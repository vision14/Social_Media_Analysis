{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45843268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c54eef9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pooja\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "import re\n",
    "import preprocessor as p\n",
    "import emoji\n",
    "import codecs\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stopwords = set(stopwords.words('english')) - set(['no', 'not'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc1c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb862f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec_model(X, y):\n",
    "    print(\"Preparing tagged data...\")\n",
    "    tagged_data = []\n",
    "    for i in tqdm(range(len(X))):\n",
    "        text = X[i]\n",
    "        tagged_data.append(TaggedDocument(words=word_tokenize(text), tags=[str(i)]))\n",
    "    print(\"Preparing Vectors...\")\n",
    "    max_epochs = 10\n",
    "    vec_size = 20\n",
    "    alpha = 0.025\n",
    "    model = Doc2Vec(vector_size=vec_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm=0)\n",
    "    model.build_vocab(tagged_data)\n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        model.train(tagged_data,total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        model.alpha -= 0.0002\n",
    "        model.min_alpha = model.alpha\n",
    "    vector_list = []\n",
    "    for i in range(len(model.dv)):\n",
    "        vector_list.append(model.dv[i])\n",
    "    print(\"Training the Model...\")\n",
    "    X = np.array(vector_list)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    print(\"Model Training Finished.\")\n",
    "    return cm, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2794ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3594c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11970, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4772ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @notarealnun: Please don't vote for the guy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OpChemtrails: Climate change models are rigged...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @RichardCowley2: New Mann-made global warmi...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @goodoldcatchy: All terrorism is a threat, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @StephenSchlegel: she's thinking about how ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  RT @notarealnun: Please don't vote for the guy...          1\n",
       "1  OpChemtrails: Climate change models are rigged...         -1\n",
       "2  RT @RichardCowley2: New Mann-made global warmi...         -1\n",
       "3  RT @goodoldcatchy: All terrorism is a threat, ...          0\n",
       "4  RT @StephenSchlegel: she's thinking about how ...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5385a53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  0], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"Sentiment\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "730f6a5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    3990\n",
       "-1    3990\n",
       " 0    3990\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32cc1fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing tagged data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11970/11970 [00:04<00:00, 2597.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:11<00:00,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Model...\n",
      "Model Training Finished.\n",
      "Confusion Matrix:\n",
      "[[880 375  89]\n",
      " [309 719 234]\n",
      " [123 259 963]]\n",
      "Accuracy:\n",
      "0.6484434320425209\n",
      "F1-Score:\n",
      "0.6484434320425209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cm, accuracy, f1 = doc2vec_model(dataset.loc[:, \"Text\"].values, dataset.loc[:, \"Sentiment\"].values)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy)\n",
    "print(\"F1-Score:\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c89a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('sentiment.csv')\n",
    "for i, v in enumerate(dataset.loc[:, \"Text\"].values):\n",
    "    v = v.lower()\n",
    "    dataset.loc[i, \"Text\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "821a738b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing tagged data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11970/11970 [00:04<00:00, 2765.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:12<00:00,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Model...\n",
      "Model Training Finished.\n",
      "Confusion Matrix:\n",
      "[[874 359 111]\n",
      " [309 708 245]\n",
      " [128 271 946]]\n",
      "Accuracy:\n",
      "0.6398380156922299\n",
      "F1-Score:\n",
      "0.6398380156922299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cm, accuracy, f1 = doc2vec_model(dataset.loc[:, \"Text\"].values, dataset.loc[:, \"Sentiment\"].values)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy)\n",
    "print(\"F1-Score:\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f42033a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('sentiment.csv')\n",
    "for i, v in enumerate(dataset.loc[:, \"Text\"].values):\n",
    "    v = v.replace('#', ' ')\n",
    "    v = v.replace('@', ' ')\n",
    "    v = p.clean(v)\n",
    "    dataset.loc[i, \"Text\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60468013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing tagged data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11970/11970 [00:04<00:00, 2798.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:10<00:00,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Model...\n",
      "Model Training Finished.\n",
      "Confusion Matrix:\n",
      "[[884 354 106]\n",
      " [331 687 244]\n",
      " [157 295 893]]\n",
      "Accuracy:\n",
      "0.6236395849152113\n",
      "F1-Score:\n",
      "0.6236395849152113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cm, accuracy, f1 = doc2vec_model(dataset.loc[:, \"Text\"].values, dataset.loc[:, \"Sentiment\"].values)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy)\n",
    "print(\"F1-Score:\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a44acebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('sentiment.csv')\n",
    "for i, v in enumerate(dataset.loc[:, \"Text\"].values):\n",
    "    v = v.replace(\"’\", \"'\")\n",
    "    for key in contractions:\n",
    "        if key in v:\n",
    "            v = re.sub(key, contractions[key], v)\n",
    "    dataset.loc[i, \"Text\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e3c355a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing tagged data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11970/11970 [00:04<00:00, 2444.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:12<00:00,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Model...\n",
      "Model Training Finished.\n",
      "Confusion Matrix:\n",
      "[[857 378 109]\n",
      " [305 718 239]\n",
      " [143 237 965]]\n",
      "Accuracy:\n",
      "0.6428752214629208\n",
      "F1-Score:\n",
      "0.6428752214629208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cm, accuracy, f1 = doc2vec_model(dataset.loc[:, \"Text\"].values, dataset.loc[:, \"Sentiment\"].values)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy)\n",
    "print(\"F1-Score:\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ab4bf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pooja\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: invalid escape sequence '\\ '\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('sentiment.csv')\n",
    "for i, v in enumerate(dataset.loc[:, \"Text\"].values):\n",
    "    v = codecs.decode(v, 'unicode_escape')\n",
    "    v = emoji.demojize(v.encode('utf-16', 'surrogatepass').decode('utf-16'))\n",
    "    dataset.loc[i, \"Text\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8ea45d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing tagged data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11970/11970 [00:05<00:00, 2270.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:12<00:00,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Model...\n",
      "Model Training Finished.\n",
      "Confusion Matrix:\n",
      "[[899 327 118]\n",
      " [293 733 236]\n",
      " [111 253 981]]\n",
      "Accuracy:\n",
      "0.6613515565679575\n",
      "F1-Score:\n",
      "0.6613515565679575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cm, accuracy, f1 = doc2vec_model(dataset.loc[:, \"Text\"].values, dataset.loc[:, \"Sentiment\"].values)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy)\n",
    "print(\"F1-Score:\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bf01597",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('sentiment.csv')\n",
    "for i, v in enumerate(dataset.loc[:, \"Text\"].values):\n",
    "    v = re.sub(\"[^a-z0-9 ]\", \" \", v).strip()\n",
    "    v = re.sub(\" +\", \" \", v)\n",
    "    dataset.loc[i, \"Text\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04d0a0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing tagged data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11970/11970 [00:02<00:00, 4281.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:13<00:00,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Model...\n",
      "Model Training Finished.\n",
      "Confusion Matrix:\n",
      "[[852 375 117]\n",
      " [320 692 250]\n",
      " [162 305 878]]\n",
      "Accuracy:\n",
      "0.613009364717793\n",
      "F1-Score:\n",
      "0.613009364717793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cm, accuracy, f1 = doc2vec_model(dataset.loc[:, \"Text\"].values, dataset.loc[:, \"Sentiment\"].values)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy)\n",
    "print(\"F1-Score:\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60de328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('sentiment.csv')\n",
    "for i, v in enumerate(dataset.loc[:, \"Text\"].values):\n",
    "    temp_v = v.split()\n",
    "    l = WordNetLemmatizer()\n",
    "    v = [l.lemmatize(word) for word in temp_v if not word in stopwords]\n",
    "    v = ' '.join(v)\n",
    "    dataset.loc[i, \"Text\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a7fdd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing tagged data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11970/11970 [00:04<00:00, 2577.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:07<00:00,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Model...\n",
      "Model Training Finished.\n",
      "Confusion Matrix:\n",
      "[[886 354 104]\n",
      " [294 663 305]\n",
      " [138 270 937]]\n",
      "Accuracy:\n",
      "0.6292077954948114\n",
      "F1-Score:\n",
      "0.6292077954948114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cm, accuracy, f1 = doc2vec_model(dataset.loc[:, \"Text\"].values, dataset.loc[:, \"Sentiment\"].values)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy)\n",
    "print(\"F1-Score:\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "440b6eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pooja\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: invalid escape sequence '\\ '\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('sentiment.csv')\n",
    "for i, v in enumerate(dataset.loc[:, \"Text\"].values):\n",
    "    v = v.replace(\"’\", \"'\")\n",
    "    for key in contractions:\n",
    "        if key in v:\n",
    "            v = re.sub(key, contractions[key], v)\n",
    "    v = codecs.decode(v, 'unicode_escape')\n",
    "    v = emoji.demojize(v.encode('utf-16', 'surrogatepass').decode('utf-16'))\n",
    "    dataset.loc[i, \"Text\"] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7947697f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing tagged data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11970/11970 [00:02<00:00, 4284.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:46<00:00,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Model...\n",
      "Model Training Finished.\n",
      "Confusion Matrix:\n",
      "[[871 361 112]\n",
      " [284 748 230]\n",
      " [135 263 947]]\n",
      "Accuracy:\n",
      "0.6494558339660845\n",
      "F1-Score:\n",
      "0.6494558339660845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cm, accuracy, f1 = doc2vec_model(dataset.loc[:, \"Text\"].values, dataset.loc[:, \"Sentiment\"].values)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy)\n",
    "print(\"F1-Score:\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ac3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
